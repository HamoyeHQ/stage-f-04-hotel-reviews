{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /opt/conda/lib/python3.7/site-packages (20.2.3)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --user --upgrade pip\n",
    "\n",
    "!pip3 install pandas==0.23.4 matplotlib==3.0.3 scipy==1.2.1 scikit-learn==0.22 tensorflow==2.0 keras==1.2.2 --user --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install kfp --upgrade --user --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Kubeflow SDK\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the outputs are stored\n",
    "out_dir = \"https://storage.googleapis.com/micro-access-290111/Hotel_Pipeline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from nltk.corpus import stopwords\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import linear_kernel\n",
    "    from joblib import load,dump\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.22'])\n",
    "    out_dir = \"https://storage.googleapis.com/micro-access-290111/Hotel_Pipeline\"\n",
    "    \n",
    "    df1 = pd.read_csv(\"https://storage.googleapis.com/micro-access-290111/Hotel_Review_Dataset(1).csv\")\n",
    "    df2 = pd.read_csv(\"https://storage.googleapis.com/micro-access-290111/Hotel_Review_Dataset(2).csv\")\n",
    "    df3 = pd.read_csv(\"https://storage.googleapis.com/micro-access-290111/Hotel_Review_Dataset(3).csv\")\n",
    "    \n",
    "    df = pd.concat([df1,df2,df3])\n",
    "    \n",
    "    class Review_recommender():\n",
    "        def __init__(self,data=df):\n",
    "            self.data = data\n",
    "            self.clean_df = self.data.groupby('Hotel_Name').agg({'Negative_Review':', '.join,'Positive_Review':', '.join}).reset_index()\n",
    "            self.clean_df['Combined_Review'] = self.clean_df['Positive_Review'].astype(str) + self.clean_df['Negative_Review'].astype(str)\n",
    "            self.clean_df[\"Combined_Review\"] = self.clean_df[\"Combined_Review\"].apply(lambda x: x.replace(\"No Negative\", \"\").replace(\"No Positive\", \"\"))\n",
    "            self.clean_df['Combined_Review'] = self.clean_df['Combined_Review'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "            stop = stopwords.words('english')\n",
    "            self.clean_df['Combined_Review'] = self.clean_df['Combined_Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "            tfv = TfidfVectorizer(min_df=3, max_features =None,strip_accents='unicode',analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1,3))\n",
    "            self.tfv_matrix = tfv.fit_transform(self.clean_df['Combined_Review'])\n",
    "            self.clean_df.drop(['Combined_Review','Positive_Review','Negative_Review'],1,inplace=True)\n",
    "            del tfv,self.data,stop\n",
    "            self.cos_sim = linear_kernel(self.tfv_matrix, self.tfv_matrix)\n",
    "            self.indices = pd.Series(self.clean_df.index, index=self.clean_df['Hotel_Name']).drop_duplicates()\n",
    "        \n",
    "        def recommend(self,index):\n",
    "            index = self.indices[index]\n",
    "            sim_scores = list(enumerate(self.cos_sim[index]))\n",
    "            sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "            sim_scores = sim_scores[1:11]\n",
    "            hotel_indices = [i[0] for i in sim_scores]\n",
    "            for i in self.clean_df['Hotel_Name'].iloc[hotel_indices]:\n",
    "                print(i)\n",
    "    \n",
    "        name = \"Hotel Arena\"\n",
    "        recommender = Review_recommender()\n",
    "        recommender.recommend(name)\n",
    "        \n",
    "        dump(recommender,'{}/model/recommender_based_on_reviews.joblib'.format(out_dir))\n",
    "        print(\"Recommendation was successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_b():\n",
    "    # Importing needed libraries\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from ast import literal_eval\n",
    "    from nltk.corpus import stopwords \n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    \n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.22'])\n",
    "    out_dir = \"https://storage.googleapis.com/micro-access-290111/Hotel_Pipeline\"\n",
    "    \n",
    "    df1 = pd.read_csv(\"https://storage.googleapis.com/micro-access-290111/Hotel_Review_Dataset(1).csv\")\n",
    "    df2 = pd.read_csv(\"https://storage.googleapis.com/micro-access-290111/Hotel_Review_Dataset(2).csv\")\n",
    "    df3 = pd.read_csv(\"https://storage.googleapis.com/micro-access-290111/Hotel_Review_Dataset(3).csv\")\n",
    "    \n",
    "    df = pd.concat([df1,df2,df3])\n",
    "    \n",
    "    \n",
    "    # Dropping unneeded columns\n",
    "    df.drop(['Unnamed: 0', 'Additional_Number_of_Scoring',\n",
    "       'Review_Date','Reviewer_Nationality',\n",
    "       'Negative_Review', 'Review_Total_Negative_Word_Counts',\n",
    "       'Total_Number_of_Reviews', 'Positive_Review',\n",
    "       'Review_Total_Positive_Word_Counts',\n",
    "       'Total_Number_of_Reviews_Reviewer_Has_Given', 'Reviewer_Score',\n",
    "       'days_since_review', 'lat', 'lng', 'City', 'tourist',\n",
    "       'Trip_type', 'Travelling_Status', 'stay_duration', 'room_small',\n",
    "       'wi_fi', 'air_conditioning', 'breakfast', 'booking_com', 'room_problem',\n",
    "       'location', 'staff', 'bed_and_room', 'month', 'year'],1,inplace=True)\n",
    "    \n",
    "\n",
    "    class Tags_Country_recommender():\n",
    "        def __init__(self,data=df):\n",
    "            self.data = data\n",
    "            # Seperating the tags components to make it clearer\n",
    "            self.data['Tags'] = self.data['Tags'].apply(lambda x:\"\".join(literal_eval(x)))\n",
    "            # Making these columns lowercase\n",
    "            self.data['Country']=self.data['Country'].str.lower()\n",
    "            self.data['Tags']=self.data['Tags'].str.lower()\n",
    "      \n",
    "        def recommend(self,location,description):   \n",
    "            # Dividing the texts into small tokens (sentences into words)\n",
    "            description = description.lower()\n",
    "            description_tokens=word_tokenize(description)  \n",
    "            sw = stopwords.words('english')  # List of predefined english stopwords to be used for computing\n",
    "            lemm = WordNetLemmatizer() # This groups similar words so that it can be analyzed as a single word\n",
    "        \n",
    "            # We now define the functions below connecting these imported packages\n",
    "            filtered_sen = {w for w in description_tokens if not w in sw}\n",
    "            f_set=set()\n",
    "            for fs in filtered_sen:\n",
    "                f_set.add(lemm.lemmatize(fs))\n",
    "        \n",
    "        \n",
    "        # Defining a new variable that takes in the location inputted and bring out the features defined below\n",
    "        country_feat = self.data[self.data['Country']==location.lower()]\n",
    "        country_feat = country_feat.set_index(np.arange(country_feat.shape[0]))\n",
    "        cos=[]\n",
    "        for i in range(country_feat.shape[0]):\n",
    "            country_tokens=word_tokenize(country_feat['Tags'][i])\n",
    "            filtered_set={w for w in country_tokens if not w in sw}\n",
    "            t_set=set()\n",
    "            for te in filtered_set:\n",
    "                t_set.add(lemm.lemmatize(te))\n",
    "            rvector = t_set.intersection(f_set)\n",
    "            cos.append(len(rvector))\n",
    "        country_feat['similarity']=cos\n",
    "        country_feat=country_feat.sort_values(by='similarity',ascending=False)\n",
    "        country_feat.drop_duplicates(subset='Hotel_Name',keep='first',inplace=True)\n",
    "        country_feat.sort_values('Average_Score',ascending=False,inplace=True)\n",
    "        country_feat.reset_index(inplace=True)\n",
    "        # Printing top 10 recommendations based on the country and descriptions given\n",
    "        # Prints out both the hotel name and its location\n",
    "        for i in range(10):\n",
    "            print (f'We recommend {country_feat.iloc[i,3]} located at {country_feat.iloc[i,1]}')   \n",
    "            \n",
    "        \n",
    "        recommender = Tags_Country_recommender()\n",
    "        recommender.recommend('Netherlands','I am going on a business trip, I need a standard room and i am staying for two nights ')\n",
    "        # Saving the model as a joblib file\n",
    "        from joblib import load,dump\n",
    "\n",
    "        dump(recommender,'https://storage.googleapis.com/micro-access-290111/Hotel_Pipeline/model/recommender_based_tags_and_countries.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train lightweight components.\n",
    "train_op = comp.func_to_container_op(train , base_image = \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "train_b_op = comp.func_to_container_op(train_b , base_image = \"tensorflow/tensorflow:latest-gpu-py3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "@dsl.pipeline(\n",
    "   name='Hotel Recommendation pipeline',\n",
    "   description='A recommender system pipeline that performs recommendation system algorithms.'\n",
    ")\n",
    "def reco_container_pipeline():\n",
    "    train_task = train_op\n",
    "    train_b_task = train_b_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"http://6de007513ae0f025-dot-us-central2.pipelines.googleusercontent.com/#/experiments/details/1183f21e-21ec-4901-a2de-790efe926196\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-582e11cca2a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#Submit a pipeline run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mkfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'6de007513ae0f025-dot-us-central2.pipelines.googleusercontent.com'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_run_from_pipeline_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreco_container_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/kfp/_client.py\u001b[0m in \u001b[0;36mcreate_run_from_pipeline_func\u001b[0;34m(self, pipeline_func, arguments, run_name, experiment_name, pipeline_conf, namespace)\u001b[0m\n\u001b[1;32m    564\u001b[0m       \u001b[0mpipeline_package_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pipeline.yaml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m       \u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline_package_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline_conf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline_conf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_run_from_pipeline_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_package_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcreate_run_from_pipeline_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/kfp/_client.py\u001b[0m in \u001b[0;36mcreate_run_from_pipeline_package\u001b[0;34m(self, pipeline_file, arguments, run_name, experiment_name, namespace)\u001b[0m\n\u001b[1;32m    605\u001b[0m                                 '%Y-%m-%d %H-%M-%S'))\n\u001b[1;32m    606\u001b[0m     \u001b[0mexperiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperiment_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m     \u001b[0mrun_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mRunPipelineResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/kfp/_client.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(self, experiment_id, job_name, pipeline_package_path, params, pipeline_id, version_id)\u001b[0m\n\u001b[1;32m    427\u001b[0m       \u001b[0mpipeline_package_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline_package_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m       \u001b[0mpipeline_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m       version_id=version_id)\n\u001b[0m\u001b[1;32m    430\u001b[0m     run_body = kfp_server_api.models.ApiRun(\n\u001b[1;32m    431\u001b[0m         pipeline_spec=job_config.spec, resource_references=job_config.resource_references, name=job_name)\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/kfp/_client.py\u001b[0m in \u001b[0;36m_create_job_config\u001b[0;34m(self, experiment_id, params, pipeline_package_path, pipeline_id, version_id)\u001b[0m\n\u001b[1;32m    524\u001b[0m     api_params = [kfp_server_api.ApiParameter(\n\u001b[1;32m    525\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msanitize_k8s_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_capital_underscore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         value=str(v)) for k,v in params.items()]\n\u001b[0m\u001b[1;32m    527\u001b[0m     \u001b[0mresource_references\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m     key = kfp_server_api.models.ApiResourceKey(id=experiment_id,\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "#Specify pipeline argument values\n",
    "arguments = out_dir\n",
    "pipeline_func = reco_container_pipeline\n",
    "experiment_name = 'hotel_recommender_kubeflow'\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func, '{}.zip'.format(experiment_name))\n",
    "\n",
    "#Submit a pipeline run\n",
    "kfp.Client(host='6de007513ae0f025-dot-us-central2.pipelines.googleusercontent.com').create_run_from_pipeline_func(reco_container_pipeline, arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
